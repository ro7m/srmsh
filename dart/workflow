// main.dart
import 'dart:convert';
import 'dart:io';
import 'package:flutter/services.dart';
import 'package:flutter/material.dart';
import 'package:path_provider/path_provider.dart';

void main() async {
  WidgetsFlutterBinding.ensureInitialized();
  await copyModelsToDocuments();
  runApp(const MyApp());
}

Future<void> copyModelsToDocuments() async {
  final appDir = await getApplicationDocumentsDirectory();
  final modelsDir = Directory('${appDir.path}/assets/models');
  await modelsDir.create(recursive: true);

  final manifestContent = await rootBundle.loadString('AssetManifest.json');
  final Map<String, dynamic> manifest = json.decode(manifestContent);
  
  for (String path in manifest.keys) {
    if (path.startsWith('assets/models/')) {
      final filename = path.split('/').last;
      final bytes = await rootBundle.load(path);
      final buffer = bytes.buffer;
      await File('${modelsDir.path}/$filename')
          .writeAsBytes(buffer.asUint8List(bytes.offsetInBytes, bytes.lengthInBytes));
    }
  }
}

// ocr_service.dart
import 'dart:typed_data';
import 'dart:ui' as ui;
import 'dart:math' show exp, max;
import 'dart:io';
import 'package:flutter/material.dart';
import 'package:image/image.dart' as img_lib;
import 'package:path_provider/path_provider.dart';
import 'package:onnxruntime/onnxruntime.dart';
import 'package:opencv_dart/opencv_dart.dart' as cv;

class OCRService {
  OrtSession? detectionModel;
  OrtSession? recognitionModel;
  
  Future<void> loadModels() async {
    try {
      final appDir = await getApplicationDocumentsDirectory();
      
      // Initialize ONNX Runtime
      final options = OrtSessionOptions();
      options.setIntraOpNumThreads(1);
      options.setInterOpNumThreads(1);
      
      // Load detection model
      final detectionFile = File('${appDir.path}/assets/models/rep_fast_base.onnx');
      detectionModel = await OrtSession.fromFile(detectionFile, options);
      
      // Load recognition model
      final recognitionFile = File('${appDir.path}/assets/models/crnn_mobilenet_v3_large.onnx');
      recognitionModel = await OrtSession.fromFile(recognitionFile, options);
      
    } catch (e) {
      throw Exception('Error loading models: $e');
    }
  }

  Future<Float32List> preprocessImageForDetection(ui.Image image) async {
    final img = await uiImageToImage(image);
    if (img == null) throw Exception('Failed to process image');

    final resized = img_lib.copyResize(
      img,
      width: OCRConstants.TARGET_SIZE[0],
      height: OCRConstants.TARGET_SIZE[1],
    );

    final preprocessedData = Float32List(OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 3);
    
    for (int y = 0; y < resized.height; y++) {
      for (int x = 0; x < resized.width; x++) {
        final color = img_lib.getColor(resized, x, y);
        final idx = y * resized.width + x;
        
        preprocessedData[idx] = 
            (color.r / 255.0 - OCRConstants.DET_MEAN[0]) / OCRConstants.DET_STD[0];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1]] = 
            (color.g / 255.0 - OCRConstants.DET_MEAN[1]) / OCRConstants.DET_STD[1];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 2] = 
            (color.b / 255.0 - OCRConstants.DET_MEAN[2]) / OCRConstants.DET_STD[2];
      }
    }
    
    return preprocessedData;
  }

  Future<Map<String, dynamic>> detectText(ui.Image image) async {
    if (detectionModel == null) throw Exception('Detection model not loaded');
    
    try {
      final inputTensor = await preprocessImageForDetection(image);
      
      // Create ONNX tensor
      final tensor = OrtTensor.fromList(
        TensorElementType.float,
        inputTensor,
        [1, 3, OCRConstants.TARGET_SIZE[0], OCRConstants.TARGET_SIZE[1]]
      );

      final feeds = {'input': tensor};
      final results = await detectionModel!.run(feeds);
      final probMap = results.values.first.value as Float32List;
      
      final processedProbMap = Float32List.fromList(
        probMap.map((x) => 1.0 / (1.0 + exp(-x))).toList()
      );

      return {
        'out_map': processedProbMap,
        'preds': postprocessProbabilityMap(processedProbMap),
      };
    } catch (e) {
      throw Exception('Error running detection model: $e');
    }
  }

  Future<List<BoundingBox>> extractBoundingBoxes(Float32List probMap) async {
    final imgWidth = OCRConstants.TARGET_SIZE[0];
    final imgHeight = OCRConstants.TARGET_SIZE[1];
    
    try {
      // Convert to OpenCV matrix
      final matData = probMap.map((x) => (x * 255).toInt().clamp(0, 255)).toList();
      final mat = cv.Mat.create(imgHeight, imgWidth, cv.CV_8UC1);
      mat.data = matData;

      // Apply threshold
      final binary = cv.Mat.create(imgHeight, imgWidth, cv.CV_8UC1);
      cv.threshold(mat, binary, 77, 255, cv.THRESH_BINARY);
      
      // Find contours
      final List<List<cv.Point>> contours = [];
      final hierarchy = cv.Mat.create(1, 1, cv.CV_32SC4);
      cv.findContours(binary, contours, hierarchy, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE);

      final boundingBoxes = <BoundingBox>[];
      
      for (final contour in contours) {
        final rect = cv.boundingRect(contour);
        if (rect.width > 2 && rect.height > 2) {
          boundingBoxes.add(BoundingBox(
            x: rect.x.toDouble(),
            y: rect.y.toDouble(),
            width: rect.width.toDouble(),
            height: rect.height.toDouble(),
          ));
        }
      }

      mat.release();
      binary.release();
      hierarchy.release();

      return boundingBoxes;
    } catch (e) {
      throw Exception('Error extracting bounding boxes: $e');
    }
  }

  // ... Rest of the implementation remains the same ...
}
