import 'dart:typed_data';
import 'dart:ui' as ui;
import 'dart:math' show exp, max;
import 'dart:io';
import 'package:flutter/material.dart';
import 'package:image/image.dart' as img_lib;
import 'package:path_provider/path_provider.dart';
import 'package:onnxruntime/onnxruntime.dart';
import 'package:opencv_dart/opencv_dart.dart' as cv;
import '../constants.dart';
import '../models/bounding_box.dart';

class OCRResult {
  final String text;
  final BoundingBox boundingBox;

  OCRResult({required this.text, required this.boundingBox});
}

class OCRService {
  OrtSession? detectionModel;
  OrtSession? recognitionModel;
  late OrtEnv env;
  
  Future<void> loadModels() async {
    try {
      // Get the application documents directory
      final appDir = await getApplicationDocumentsDirectory();
      
      // Create OrtEnv
      env = OrtEnv(identifier: 'ocr_env');
      
      // Load detection model
      final detectionFile = File('${appDir.path}/assets/models/rep_fast_base.onnx');
      detectionModel = OrtSession.fromFile(
        detectionFile,
        OrtSessionOptions()
          ..setIntraOpNumThreads(1)
          ..setInterOpNumThreads(1),
      );
      
      // Load recognition model
      final recognitionFile = File('${appDir.path}/assets/models/crnn_mobilenet_v3_large.onnx');
      recognitionModel = OrtSession.fromFile(
        recognitionFile,
        OrtSessionOptions()
          ..setIntraOpNumThreads(1)
          ..setInterOpNumThreads(1),
      );
      
      if (detectionModel == null || recognitionModel == null) {
        throw Exception('Failed to load ONNX models');
      }
    } catch (e) {
      throw Exception('Error loading models: $e');
    }
  }

  Future<img_lib.Image?> uiImageToImage(ui.Image image) async {
    try {
      final ByteData? byteData = await image.toByteData(format: ui.ImageByteFormat.rawRgba);
      if (byteData == null) return null;

      return img_lib.Image.fromBytes(
        width: image.width,
        height: image.height,
        bytes: byteData.buffer,
        numChannels: 4,
      );
    } catch (e) {
      throw Exception('Error converting UI Image: $e');
    }
  }

  Future<Float32List> preprocessImageForDetection(ui.Image image) async {
    final img_lib.Image? processedImage = await uiImageToImage(image);
    if (processedImage == null) throw Exception('Failed to process image');

    final resized = img_lib.copyResize(
      processedImage,
      width: OCRConstants.TARGET_SIZE[0],
      height: OCRConstants.TARGET_SIZE[1],
    );

    final Float32List preprocessedData = Float32List(OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 3);
    
    for (int y = 0; y < resized.height; y++) {
      for (int x = 0; x < resized.width; x++) {
        final pixel = resized.getPixel(x, y);
        final int idx = y * resized.width + x;
        
        // Use correct color channel extraction methods
        preprocessedData[idx] = 
            ((pixel >> 16 & 0xFF) / 255.0 - OCRConstants.DET_MEAN[0]) / OCRConstants.DET_STD[0];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1]] = 
            ((pixel >> 8 & 0xFF) / 255.0 - OCRConstants.DET_MEAN[1]) / OCRConstants.DET_STD[1];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 2] = 
            ((pixel & 0xFF) / 255.0 - OCRConstants.DET_MEAN[2]) / OCRConstants.DET_STD[2];
      }
    }
    
    return preprocessedData;
  }

  Future<Map<String, dynamic>> detectText(ui.Image image) async {
    if (detectionModel == null) throw Exception('Detection model not loaded');
    
    try {
      final inputTensor = await preprocessImageForDetection(image);
      
      // Create input tensor using OrtValueTensor constructor
      final inputOrtTensor = OrtValueTensor.fromList(
        inputTensor,
        [1, 3, OCRConstants.TARGET_SIZE[0], OCRConstants.TARGET_SIZE[1]],
      );

      final feeds = {'input': inputOrtTensor};
      final results = await detectionModel!.run(feeds);
      final probMap = results.values.first.value as Float32List;
      
      final processedProbMap = Float32List.fromList(
        probMap.map((x) => 1.0 / (1.0 + exp(-x))).toList()
      );

      return {
        'out_map': processedProbMap,
        'preds': postprocessProbabilityMap(processedProbMap),
      };
    } catch (e) {
      throw Exception('Error running detection model: $e');
    }
  }

  Future<List<BoundingBox>> extractBoundingBoxes(Float32List probMap) async {
    final imgWidth = OCRConstants.TARGET_SIZE[0];
    final imgHeight = OCRConstants.TARGET_SIZE[1];
    
    try {
      // Convert probability map to OpenCV Mat
      final mat = cv.Mat.fromArray(
        probMap.map((x) => (x * 255).toInt().clamp(0, 255)).toList(),
        cv.CV_8UC1,
        imgHeight,
        imgWidth,
      );
      
      // Apply threshold
      final thresholdMat = cv.Mat();
      cv.threshold(mat, thresholdMat, 77, 255, cv.THRESH_BINARY);
      
      // Find contours
      final contours = <cv.Point>[];
      final hierarchy = cv.Mat();
      cv.findContours(
        thresholdMat,
        contours,
        hierarchy,
        cv.RETR_EXTERNAL,
        cv.CHAIN_APPROX_SIMPLE,
      );

      List<BoundingBox> boundingBoxes = [];
      
      for (final contour in contours) {
        final rect = cv.boundingRect(contour);
        
        if (rect.width > 2 && rect.height > 2) {
          final box = _transformBoundingBox(
            rect.x.toDouble(),
            rect.y.toDouble(),
            rect.width.toDouble(),
            rect.height.toDouble(),
            imgWidth.toDouble(),
            imgHeight.toDouble(),
          );
          boundingBoxes.add(box);
        }
      }

      // Clean up OpenCV resources
      mat.release();
      thresholdMat.release();
      hierarchy.release();

      return boundingBoxes;
    } catch (e) {
      throw Exception('Error extracting bounding boxes: $e');
    }
  }

  List<int> postprocessProbabilityMap(Float32List probMap) {
    const threshold = 0.1;
    return probMap.map((prob) => prob > threshold ? 1 : 0).toList();
  }

  // ... Rest of the implementation remains the same ...

  BoundingBox _transformBoundingBox(
    double x,
    double y,
    double width,
    double height,
    double imgWidth,
    double imgHeight,
  ) {
    final offset = (width * height * 1.8) / (2 * (width + height));
    
    final x1 = _clamp(x - offset, imgWidth);
    final x2 = _clamp(x1 + width + 2 * offset, imgWidth);
    final y1 = _clamp(y - offset, imgHeight);
    final y2 = _clamp(y1 + height + 2 * offset, imgHeight);
    
    return BoundingBox(
      x: x1,
      y: y1,
      width: x2 - x1,
      height: y2 - y1,
    );
  }

  double _clamp(double value, double max) {
    return value.clamp(0, max);
  }
}

_---------------------____-----____
import 'dart:typed_data';
import 'dart:ui' as ui;
import 'dart:math' show exp, max;
import 'dart:io';
import 'package:flutter/material.dart';
import 'package:image/image.dart' as img_lib;
import 'package:path_provider/path_provider.dart';
import 'package:onnxruntime/onnxruntime.dart';
import 'package:opencv_4/opencv_4.dart';
import '../constants.dart';
import '../models/bounding_box.dart';

class OCRResult {
  final String text;
  final BoundingBox boundingBox;

  OCRResult({required this.text, required this.boundingBox});
}

class OCRService {
  OrtSession? detectionModel;
  OrtSession? recognitionModel;
  late OrtEnv env;
  
  Future<void> loadModels() async {
    try {
      // Get the application documents directory
      final appDir = await getApplicationDocumentsDirectory();
      
      // Create OrtEnv
      env = OrtEnv(identifier: 'ocr_env');
      
      // Load detection model
      final detectionFile = File('${appDir.path}/assets/models/rep_fast_base.onnx');
      detectionModel = OrtSession.fromFile(
        detectionFile,
        OrtSessionOptions()
          ..setIntraOpNumThreads(1)
          ..setInterOpNumThreads(1),
      );
      
      // Load recognition model
      final recognitionFile = File('${appDir.path}/assets/models/crnn_mobilenet_v3_large.onnx');
      recognitionModel = OrtSession.fromFile(
        recognitionFile,
        OrtSessionOptions()
          ..setIntraOpNumThreads(1)
          ..setInterOpNumThreads(1),
      );
      
      if (detectionModel == null || recognitionModel == null) {
        throw Exception('Failed to load ONNX models');
      }
    } catch (e) {
      throw Exception('Error loading models: $e');
    }
  }

  Future<img_lib.Image?> uiImageToImage(ui.Image image) async {
    try {
      final ByteData? byteData = await image.toByteData(format: ui.ImageByteFormat.rawRgba);
      if (byteData == null) return null;

      return img_lib.Image.fromBytes(
        width: image.width,
        height: image.height,
        bytes: byteData.buffer,
        numChannels: 4,
      );
    } catch (e) {
      throw Exception('Error converting UI Image: $e');
    }
  }

  Future<Float32List> preprocessImageForDetection(ui.Image image) async {
    final img_lib.Image? processedImage = await uiImageToImage(image);
    if (processedImage == null) throw Exception('Failed to process image');

    final resized = img_lib.copyResize(
      processedImage,
      width: OCRConstants.TARGET_SIZE[0],
      height: OCRConstants.TARGET_SIZE[1],
    );

    final Float32List preprocessedData = Float32List(OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 3);
    
    for (int y = 0; y < resized.height; y++) {
      for (int x = 0; x < resized.width; x++) {
        final pixel = resized.getPixel(x, y);
        final int idx = y * resized.width + x;
        
        // Use correct color channel extraction methods
        preprocessedData[idx] = 
            ((pixel >> 16 & 0xFF) / 255.0 - OCRConstants.DET_MEAN[0]) / OCRConstants.DET_STD[0];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1]] = 
            ((pixel >> 8 & 0xFF) / 255.0 - OCRConstants.DET_MEAN[1]) / OCRConstants.DET_STD[1];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 2] = 
            ((pixel & 0xFF) / 255.0 - OCRConstants.DET_MEAN[2]) / OCRConstants.DET_STD[2];
      }
    }
    
    return preprocessedData;
  }

  Future<Map<String, dynamic>> detectText(ui.Image image) async {
    if (detectionModel == null) throw Exception('Detection model not loaded');
    
    try {
      final inputTensor = await preprocessImageForDetection(image);
      
      // Create input tensor using OrtValueTensor constructor
      final inputOrtTensor = OrtValueTensor.fromList(
        inputTensor,
        [1, 3, OCRConstants.TARGET_SIZE[0], OCRConstants.TARGET_SIZE[1]],
      );

      final feeds = {'input': inputOrtTensor};
      final results = await detectionModel!.run(feeds);
      final probMap = results.values.first.value as Float32List;
      
      final processedProbMap = Float32List.fromList(
        probMap.map((x) => 1.0 / (1.0 + exp(-x))).toList()
      );

      return {
        'out_map': processedProbMap,
        'preds': postprocessProbabilityMap(processedProbMap),
      };
    } catch (e) {
      throw Exception('Error running detection model: $e');
    }
  }

  Future<List<BoundingBox>> extractBoundingBoxes(Float32List probMap) async {
    final imgWidth = OCRConstants.TARGET_SIZE[0];
    final imgHeight = OCRConstants.TARGET_SIZE[1];
    
    try {
      final byteData = Uint8List.fromList(
        probMap.map((x) => (x * 255).toInt().clamp(0, 255)).toList()
      );
      
      // Use OpenCV 4 methods
      final matrix = await ImgProc.threshold(
        byteData,
        imgWidth,
        imgHeight,
        77,
        255,
        ImgProc.THRESH_BINARY,
      );

      final contours = await ImgProc.findContours(
        matrix,
        ImgProc.RETR_EXTERNAL,
        ImgProc.CHAIN_APPROX_SIMPLE,
      );

      List<BoundingBox> boundingBoxes = [];
      
      for (final contour in contours) {
        final rect = await ImgProc.boundingRect(contour);
        
        if (rect[2] > 2 && rect[3] > 2) {
          final box = _transformBoundingBox(
            rect[0].toDouble(),
            rect[1].toDouble(),
            rect[2].toDouble(),
            rect[3].toDouble(),
            imgWidth.toDouble(),
            imgHeight.toDouble(),
          );
          boundingBoxes.add(box);
        }
      }

      return boundingBoxes;
    } catch (e) {
      throw Exception('Error extracting bounding boxes: $e');
    }
  }

  // ... Rest of the implementation remains the same ...
}


-------------------------
________&___&_______________

dependencies:
  flutter:
    sdk: flutter
  image: ^4.0.15
  path_provider: ^2.0.15
  onnxruntime: ^0.0.1
  opencv_4: ^1.0.0

-------;;;;;;;---------

Future<void> copyModelsToDocuments() async {
  final appDir = await getApplicationDocumentsDirectory();
  final modelsDir = Directory('${appDir.path}/assets/models');
  await modelsDir.create(recursive: true);

  // Copy models from assets to documents
  final manifestContent = await rootBundle.loadString('AssetManifest.json');
  final Map<String, dynamic> manifest = json.decode(manifestContent);
  
  for (String path in manifest.keys) {
    if (path.startsWith('assets/models/')) {
      final filename = path.split('/').last;
      final bytes = await rootBundle.load(path);
      final buffer = bytes.buffer;
      await File('${modelsDir.path}/$filename')
          .writeAsBytes(buffer.asUint8List(bytes.offsetInBytes, bytes.lengthInBytes));
    }
  }
}

------________--------_______-----


import 'dart:typed_data';
import 'dart:ui' as ui;
import 'dart:math' show exp, max;
import 'dart:io';
import 'package:flutter/material.dart';
import 'package:image/image.dart' as img_lib;
import 'package:path_provider/path_provider.dart';
import 'package:onnxruntime/onnxruntime.dart';
import 'package:opencv_dart/opencv_dart.dart' as cv;
import '../constants.dart';
import '../models/bounding_box.dart';

class OCRResult {
  final String text;
  final BoundingBox boundingBox;

  OCRResult({required this.text, required this.boundingBox});
}

class OCRService {
  OrtSession? detectionModel;
  OrtSession? recognitionModel;
  late OrtEnv env;
  
  Future<void> loadModels() async {
    try {
      // Get the application documents directory
      final appDir = await getApplicationDocumentsDirectory();
      
      // Create OrtEnv
      env = OrtEnv(identifier: 'ocr_env');
      
      // Load detection model
      final detectionFile = File('${appDir.path}/assets/models/rep_fast_base.onnx');
      detectionModel = OrtSession.fromFile(
        detectionFile,
        OrtSessionOptions()
          ..setIntraOpNumThreads(1)
          ..setInterOpNumThreads(1),
      );
      
      // Load recognition model
      final recognitionFile = File('${appDir.path}/assets/models/crnn_mobilenet_v3_large.onnx');
      recognitionModel = OrtSession.fromFile(
        recognitionFile,
        OrtSessionOptions()
          ..setIntraOpNumThreads(1)
          ..setInterOpNumThreads(1),
      );
      
      if (detectionModel == null || recognitionModel == null) {
        throw Exception('Failed to load ONNX models');
      }
    } catch (e) {
      throw Exception('Error loading models: $e');
    }
  }

  Future<img_lib.Image?> uiImageToImage(ui.Image image) async {
    try {
      final ByteData? byteData = await image.toByteData(format: ui.ImageByteFormat.rawRgba);
      if (byteData == null) return null;

      return img_lib.Image.fromBytes(
        width: image.width,
        height: image.height,
        bytes: byteData.buffer,
        numChannels: 4,
      );
    } catch (e) {
      throw Exception('Error converting UI Image: $e');
    }
  }

  Future<Float32List> preprocessImageForDetection(ui.Image image) async {
    final img_lib.Image? processedImage = await uiImageToImage(image);
    if (processedImage == null) throw Exception('Failed to process image');

    final resized = img_lib.copyResize(
      processedImage,
      width: OCRConstants.TARGET_SIZE[0],
      height: OCRConstants.TARGET_SIZE[1],
    );

    final Float32List preprocessedData = Float32List(OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 3);
    
    for (int y = 0; y < resized.height; y++) {
      for (int x = 0; x < resized.width; x++) {
        final pixel = resized.getPixel(x, y);
        final int idx = y * resized.width + x;
        
        // Use correct color channel extraction methods
        preprocessedData[idx] = 
            ((pixel >> 16 & 0xFF) / 255.0 - OCRConstants.DET_MEAN[0]) / OCRConstants.DET_STD[0];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1]] = 
            ((pixel >> 8 & 0xFF) / 255.0 - OCRConstants.DET_MEAN[1]) / OCRConstants.DET_STD[1];
        preprocessedData[idx + OCRConstants.TARGET_SIZE[0] * OCRConstants.TARGET_SIZE[1] * 2] = 
            ((pixel & 0xFF) / 255.0 - OCRConstants.DET_MEAN[2]) / OCRConstants.DET_STD[2];
      }
    }
    
    return preprocessedData;
  }

  Future<Map<String, dynamic>> detectText(ui.Image image) async {
    if (detectionModel == null) throw Exception('Detection model not loaded');
    
    try {
      final inputTensor = await preprocessImageForDetection(image);
      
      // Create input tensor using OrtValueTensor constructor
      final inputOrtTensor = OrtValueTensor.fromList(
        inputTensor,
        [1, 3, OCRConstants.TARGET_SIZE[0], OCRConstants.TARGET_SIZE[1]],
      );

      final feeds = {'input': inputOrtTensor};
      final results = await detectionModel!.run(feeds);
      final probMap = results.values.first.value as Float32List;
      
      final processedProbMap = Float32List.fromList(
        probMap.map((x) => 1.0 / (1.0 + exp(-x))).toList()
      );

      return {
        'out_map': processedProbMap,
        'preds': postprocessProbabilityMap(processedProbMap),
      };
    } catch (e) {
      throw Exception('Error running detection model: $e');
    }
  }

  Future<List<BoundingBox>> extractBoundingBoxes(Float32List probMap) async {
    final imgWidth = OCRConstants.TARGET_SIZE[0];
    final imgHeight = OCRConstants.TARGET_SIZE[1];
    
    try {
      // Convert probability map to OpenCV Mat
      final mat = cv.Mat.fromArray(
        probMap.map((x) => (x * 255).toInt().clamp(0, 255)).toList(),
        cv.CV_8UC1,
        imgHeight,
        imgWidth,
      );
      
      // Apply threshold
      final thresholdMat = cv.Mat();
      cv.threshold(mat, thresholdMat, 77, 255, cv.THRESH_BINARY);
      
      // Find contours
      final contours = <cv.Point>[];
      final hierarchy = cv.Mat();
      cv.findContours(
        thresholdMat,
        contours,
        hierarchy,
        cv.RETR_EXTERNAL,
        cv.CHAIN_APPROX_SIMPLE,
      );

      List<BoundingBox> boundingBoxes = [];
      
      for (final contour in contours) {
        final rect = cv.boundingRect(contour);
        
        if (rect.width > 2 && rect.height > 2) {
          final box = _transformBoundingBox(
            rect.x.toDouble(),
            rect.y.toDouble(),
            rect.width.toDouble(),
            rect.height.toDouble(),
            imgWidth.toDouble(),
            imgHeight.toDouble(),
          );
          boundingBoxes.add(box);
        }
      }

      // Clean up OpenCV resources
      mat.release();
      thresholdMat.release();
      hierarchy.release();

      return boundingBoxes;
    } catch (e) {
      throw Exception('Error extracting bounding boxes: $e');
    }
  }

  List<int> postprocessProbabilityMap(Float32List probMap) {
    const threshold = 0.1;
    return probMap.map((prob) => prob > threshold ? 1 : 0).toList();
  }

  // ... Rest of the implementation remains the same ...

  BoundingBox _transformBoundingBox(
    double x,
    double y,
    double width,
    double height,
    double imgWidth,
    double imgHeight,
  ) {
    final offset = (width * height * 1.8) / (2 * (width + height));
    
    final x1 = _clamp(x - offset, imgWidth);
    final x2 = _clamp(x1 + width + 2 * offset, imgWidth);
    final y1 = _clamp(y - offset, imgHeight);
    final y2 = _clamp(y1 + height + 2 * offset, imgHeight);
    
    return BoundingBox(
      x: x1,
      y: y1,
      width: x2 - x1,
      height: y2 - y1,
    );
  }

  double _clamp(double value, double max) {
    return value.clamp(0, max);
  }
}
