from fastapi import FastAPI, HTTPException
from typing import Optional, Dict
import duckdb
import pandas as pd
from datetime import datetime
import time
import boto3
import os
import tempfile
import atexit
import logging
from contextlib import asynccontextmanager

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class STSManager:
    def __init__(self, role_arn: str, region: str = 'us-east-1', session_name: str = 'DuckDBSession'):
        """Manage STS credentials for assuming IAM roles"""
        self.role_arn = role_arn
        self.region = region
        self.session_name = session_name
        self.credentials = None
        self.expiration = None

    def get_credentials(self):
        """Get temporary credentials using STS AssumeRole"""
        if self._should_refresh():
            # Create an STS client
            sts_client = boto3.client('sts', region_name=self.region)

            # Assume the IAM role
            response = sts_client.assume_role(
                RoleArn=self.role_arn,
                RoleSessionName=self.session_name,
                DurationSeconds=3600  # 1 hour
            )

            # Extract the temporary credentials
            self.credentials = response['Credentials']
            self.expiration = self.credentials['Expiration']
            logger.info("STS credentials refreshed successfully")

        return {
            'aws_access_key_id': self.credentials['AccessKeyId'],
            'aws_secret_access_key': self.credentials['SecretAccessKey'],
            'aws_session_token': self.credentials['SessionToken']
        }

    def _should_refresh(self):
        """Check if credentials need to be refreshed"""
        if not self.credentials or not self.expiration:
            return True

        # Add a buffer of 5 minutes before expiration
        now = datetime.now().replace(tzinfo=self.expiration.tzinfo)
        time_remaining = self.expiration - now
        return time_remaining.total_seconds() < 300  # Refresh if less than 5 minutes remaining


class DuckDBManager:
    def __init__(self, role_arn: str, region: str = 'us-east-1', cache_size_mb: int = 2000):
        """Initialize DuckDB connection with STS role assumption"""
        self.conn = None  # Will be initialized after file download
        self.sts_manager = STSManager(role_arn, region)
        self.cache_size_mb = cache_size_mb
        
        # Create a temporary directory that will be cleaned up on exit
        self.temp_dir = tempfile.mkdtemp(prefix="duckdb_cache_")
        atexit.register(self._cleanup_temp_files)
        
        # Set initial S3 credentials (needed for download)
        self._update_credentials()
        
        # The local file path will be set during download
        self.local_file_path = None
        self.index_db_path = None

    def initialize_with_s3_file(self, s3_bucket: str, s3_key: str):
        """Initialize by downloading the S3 file and setting up DuckDB"""
        # First download the file from S3
        success = self._download_s3_file(s3_bucket, s3_key)
        
        if not success:
            raise RuntimeError("Failed to download S3 file during initialization")
        
        # Now set up the DuckDB connection with the downloaded file
        self._setup_connection()
        
        # Create index for faster lookups if it's a Parquet file
        if s3_key.endswith('.parquet'):
            self._create_index_for_local_parquet()
            
        return success

    def _cleanup_temp_files(self):
        """Clean up temporary files on exit"""
        if self.temp_dir and os.path.exists(self.temp_dir):
            for filename in os.listdir(self.temp_dir):
                file_path = os.path.join(self.temp_dir, filename)
                try:
                    if os.path.isfile(file_path):
                        os.unlink(file_path)
                except Exception as e:
                    logger.error(f"Error deleting {file_path}: {e}")
            try:
                os.rmdir(self.temp_dir)
                logger.info(f"Removed temporary directory: {self.temp_dir}")
            except Exception as e:
                logger.error(f"Error removing directory {self.temp_dir}: {e}")

    def _setup_connection(self):
        """Set up the DuckDB connection with extensions and optimizations"""
        # Create or connect to the indexed database
        if self.index_db_path and os.path.exists(self.index_db_path):
            self.conn = duckdb.connect(database=self.index_db_path)
            logger.info(f"Connected to existing indexed database: {self.index_db_path}")
        else:
            # Create an in-memory connection if no indexed database
            self.conn = duckdb.connect(database=':memory:')
            
            # Install and load extensions (one-time operation)
            self.conn.execute("INSTALL httpfs")
            self.conn.execute("LOAD httpfs")

        # Configure DuckDB performance settings
        self.conn.execute(f"SET memory_limit='{self.cache_size_mb}MB'")
        self.conn.execute("SET enable_object_cache=true")
        self.conn.execute("SET threads=4")  # Adjust based on available CPU cores
        self.conn.execute("PRAGMA memory_limit='80%'")  # Use more available memory
        
        # Enable parallel processing and other optimizations
        self.conn.execute("PRAGMA enable_profiling")
        self.conn.execute("PRAGMA threads=4")
        self.conn.execute("PRAGMA force_parallelism")
        
        logger.info("DuckDB connection configured successfully")

    def _update_credentials(self):
        """Update the S3 credentials in DuckDB using STS"""
        credentials = self.sts_manager.get_credentials()
        
        # Store for S3 client usage (for download)
        self.current_credentials = credentials
        
        # Only update the connection if it exists
        if self.conn:
            # Configure S3 credentials in DuckDB
            self.conn.execute(f"SET s3_region='{self.sts_manager.region}'")
            self.conn.execute(f"SET s3_access_key_id='{credentials['aws_access_key_id']}'")
            self.conn.execute(f"SET s3_secret_access_key='{credentials['aws_secret_access_key']}'")
            self.conn.execute(f"SET s3_session_token='{credentials['aws_session_token']}'")

    def _download_s3_file(self, s3_bucket: str, s3_key: str) -> bool:
        """Download the S3 file to local filesystem for faster access"""
        try:
            start_time = time.time()
            logger.info(f"Starting download of s3://{s3_bucket}/{s3_key}")
            
            # Create S3 client with temporary credentials
            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.current_credentials['aws_access_key_id'],
                aws_secret_access_key=self.current_credentials['aws_secret_access_key'],
                aws_session_token=self.current_credentials['aws_session_token'],
                region_name=self.sts_manager.region
            )
            
            # Create local file path
            file_name = os.path.basename(s3_key)
            self.local_file_path = os.path.join(self.temp_dir, file_name)
            
            # Download the file
            s3_client.download_file(s3_bucket, s3_key, self.local_file_path)
            
            download_time = time.time() - start_time
            logger.info(f"File downloaded to {self.local_file_path} in {download_time:.2f} seconds")
            
            return True
                
        except Exception as e:
            logger.error(f"Error downloading S3 file: {str(e)}")
            self.local_file_path = None
            return False
    
    def _create_index_for_local_parquet(self):
        """Create an index on the ID column for faster lookups"""
        try:
            start_time = time.time()
            logger.info("Creating indexed database for faster lookups")
            
            # Create a file-based database specifically for this data
            self.index_db_path = os.path.join(self.temp_dir, "indexed_data.duckdb")
            
            # Close existing connection if any
            if self.conn:
                self.conn.close()
                
            # Create new connection to the index database
            index_conn = duckdb.connect(database=self.index_db_path)
            
            # Import the parquet file into a table
            index_conn.execute(f"""
                CREATE TABLE IF NOT EXISTS lookup_data AS 
                SELECT * FROM read_parquet('{self.local_file_path}')
            """)
            
            # Create an index on the ID column
            index_conn.execute("CREATE INDEX IF NOT EXISTS id_idx ON lookup_data(id)")
            
            # Close and reopen as our main connection
            index_conn.close()
            self.conn = duckdb.connect(database=self.index_db_path)
            
            index_time = time.time() - start_time
            logger.info(f"Created indexed database at {self.index_db_path} in {index_time:.2f} seconds")
            
            return True
            
        except Exception as e:
            logger.error(f"Error creating index: {str(e)}")
            # Fall back to memory connection with direct file access
            self.conn = duckdb.connect(database=':memory:')
            self._setup_connection()
            return False

    def execute_query(self, query: str, params: Optional[Dict] = None) -> tuple[pd.DataFrame, Dict]:
        """Execute a query with automatic credential refresh"""
        # Refresh credentials if needed
        self._update_credentials()
        
        start_time = time.time()

        try:
            # If we have a local file, modify the query to use it instead of S3
            if self.local_file_path and "read_parquet('s3://" in query:
                if self.index_db_path and "lookup_data" in self.conn.execute("SELECT table_name FROM information_schema.tables").df()['table_name'].values:
                    # Use the indexed table directly
                    query = query.replace(
                        "read_parquet('s3://",
                        "lookup_data /* replacing read_parquet('s3://"
                    )
                    query = query.replace("')", "') */")
                else:
                    # Use local parquet file
                    query = query.replace(
                        "read_parquet('s3://",
                        f"read_parquet('{self.local_file_path}' /* replacing s3://"
                    )
                    query = query.replace("')", "') */")

            # Execute query with parameters if provided
            if params:
                result = self.conn.execute(query, params).df()
            else:
                result = self.conn.execute(query).df()

            query_time = time.time() - start_time

            # Simple metrics
            metrics = {
                "query_time_ms": round(query_time * 1000, 2),
                "used_local_file": self.local_file_path is not None,
                "used_indexed_table": self.index_db_path is not None
            }
            
            logger.info(f"Query executed in {metrics['query_time_ms']} ms using {'indexed table' if metrics['used_indexed_table'] else 'local file' if metrics['used_local_file'] else 'S3 directly'}")
            return result, metrics

        except Exception as e:
            logger.error(f"Query execution failed: {str(e)}")
            raise


# FastAPI application with lifespan initialization
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize components before the application starts and clean up after it stops"""
    # Configuration values (would typically come from environment variables)
    role_config = {
        'role_arn': 'arn:aws:iam::123456789012:role/S3AccessRole',
        'region': 'us-east-1'
    }
    
    s3_config = {
        'bucket': 'your-bucket',
        'key': 'data.parquet'
    }
    
    # Global DB manager
    global db_manager
    
    # Initialize before yielding
    try:
        logger.info("Container starting - initializing DuckDB manager...")
        # Create the DB manager
        db_manager = DuckDBManager(
            role_arn=role_config['role_arn'],
            region=role_config['region'],
            cache_size_mb=4000  # Increased memory allocation
        )
        
        # Download S3 file and create index during initialization
        success = db_manager.initialize_with_s3_file(
            s3_bucket=s3_config['bucket'],
            s3_key=s3_config['key']
        )
        
        if not success:
            logger.error("Failed to initialize database with S3 file")
            # Continue anyway - will use S3 directly
        
        logger.info("Container initialized successfully - ready to handle requests")
    except Exception as e:
        logger.error(f"Error during application initialization: {str(e)}")
        # Continue anyway, will attempt to initialize on first request
    
    yield
    
    # Cleanup logic here when application shuts down
    logger.info("Application shutting down...")


# FastAPI Application
app = FastAPI(title="S3 DuckDB Point Lookup Service", lifespan=lifespan)

@app.get("/health")
async def health_check():
    """Simple health check endpoint"""
    health_status = {
        "status": "healthy", 
        "timestamp": datetime.now().isoformat()
    }
    
    # Add database status if available
    if 'db_manager' in globals():
        health_status["database_initialized"] = db_manager.conn is not None
        health_status["using_local_file"] = db_manager.local_file_path is not None
        health_status["using_indexed_db"] = db_manager.index_db_path is not None
    
    return health_status

@app.get("/query")
async def execute_query(query: str, params: Optional[Dict] = None):
    """Execute a query against Parquet files with local caching"""
    try:
        # Make sure db_manager is initialized
        if 'db_manager' not in globals() or db_manager is None:
            logger.error("DB manager not initialized")
            raise HTTPException(status_code=500, detail="Database not initialized")
            
        # Execute query using the existing connection
        result, metrics = db_manager.execute_query(query, params)

        return {
            "status": "success",
            "data": result.to_dict(orient='records'),
            "metrics": metrics,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error in execute_query endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/point-query/{id}")
async def point_query(id: int):
    """Optimized point query by ID"""
    query = """
    SELECT *
    FROM read_parquet('s3://your-bucket/data.parquet')
    WHERE id = ?
    LIMIT 1
    """
    try:
        # Make sure db_manager is initialized
        if 'db_manager' not in globals() or db_manager is None:
            logger.error("DB manager not initialized")
            raise HTTPException(status_code=500, detail="Database not initialized")
            
        result, metrics = db_manager.execute_query(query, params={'id': id})
        
        if len(result) == 0:
            return {
                "status": "not_found",
                "metrics": metrics,
                "timestamp": datetime.now().isoformat()
            }
            
        return {
            "status": "success",
            "data": result.to_dict(orient='records')[0] if len(result) > 0 else None,
            "metrics": metrics,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error in point_query endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
