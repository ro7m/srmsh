import dataiku
from dataikuapi.dss.project import DSSProject
import dataikuapi
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import sys
import dataikuscoring 
import requests
import os
import time
import duckdb
from datetime import datetime, timezone
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("ML_API")

# Initialize code
try:
    model_zip_folder = folders[0]  # This assumes 'folders' is defined elsewhere
    sys.path.append(model_zip_folder)
    model = dataikuscoring.load_model(f"{model_zip_folder}/weights.zip")
    logger.info(f"Model loaded successfully from {model_zip_folder}/weights.zip")
except NameError:
    logger.error("Variable 'folders' not defined. Please define it before this section.")
    raise

# Configuration
config = {
    'role_arn': 'arn:aws:iam::xxxx:role/role',
    'region': 'us-east-1',
    'bucket': 'mybuk',
    'projectkey': 'myMODEL',
    's3folder': 'V2_VW_prepared',
}


class STSManager:
    def __init__(self, role_arn: str, region: str = 'us-east-1', session_name: str = 'DuckDBSession'):
        self.role_arn = role_arn
        self.region = region
        self.session_name = session_name
        self.credentials = None
        self.expiration = None
        self._credentials_lock = threading.Lock()
        logger.info(f"STSManager initialized with role ARN: {role_arn}, region: {region}")
    
    def get_credentials(self) -> Dict[str, str]:
        """Get AWS credentials, refreshing if necessary. Thread-safe."""
        with self._credentials_lock:
            if self._should_refresh():
                logger.info("Refreshing AWS credentials...")
                try:
                    client = dataikuapi.DSSClient("http://x.y.x", "mytempo")
                    project_key = config['projectkey']
                    runnable_type = "pyrunnable_testrole_rolefetcher"
                    project = DSSProject(client, project_key)
                    macro = project.get_macro(runnable_type)
                    run_id = macro.run(wait=True)
                    self.credentials = macro.get_result(run_id, as_type='json')
                    self.expiration = self.credentials['expiration']
                    logger.info("Successfully retrieved AWS credentials")
                except Exception as e:
                    logger.error(f"Failed to get credentials: {str(e)}")
                    raise

        return {
            'aws_access_key_id': self.credentials['aws_access_key_id'],
            'aws_secret_access_key': self.credentials['aws_secret_access_key'],
            'aws_session_token': self.credentials['aws_session_token'],
            'expiration': self.credentials['expiration']
        }
    
    def _should_refresh(self) -> bool:
        """Determine if credentials need refreshing."""
        if not self.credentials:
            logger.debug("No credentials exist, requesting new ones")
            return True
            
        now = datetime.now(timezone.utc).isoformat()
        time_remaining = datetime.fromisoformat(self.expiration) - datetime.fromisoformat(now)
        seconds_remaining = time_remaining.total_seconds()
        
        logger.debug(f"Time remaining for token expiry: {seconds_remaining} seconds")
        return seconds_remaining < 300  # Refresh if less than 5 minutes remaining (increased from 60s)


class DuckDBManager:
    def __init__(self, role_arn: str, region: str = 'us-east-1', cache_size_mb: int = 2000):
        logger.info(f"Initializing DuckDBManager with {cache_size_mb}MB cache")
        self.conn = duckdb.connect(database=':memory:')
        self.sts_manager = STSManager(role_arn, region)
        self._conn_lock = threading.Lock()
        self._setup_connection(cache_size_mb)
        
    def _setup_connection(self, cache_size_mb: int):
        """Set up DuckDB connection with proper extensions and settings."""
        try:
            self.conn.install_extension(f"{model_zip_folder}/httpfs.duckdb_extension")
            self.conn.execute("LOAD httpfs")
            self.conn.execute(f"SET memory_limit='{cache_size_mb}MB'")
            self.conn.execute("SET enable_object_cache=true")
            logger.info("DuckDB connection set up successfully")
            self._update_credentials()
        except Exception as e:
            logger.error(f"Failed to set up DuckDB connection: {str(e)}")
            raise
    
    def _update_credentials(self):
        """Update AWS credentials in DuckDB connection. Thread-safe."""
        with self._conn_lock:
            credentials = self.sts_manager.get_credentials()
            self.conn.execute(f"SET s3_region='{self.sts_manager.region}'")
            self.conn.execute(f"SET s3_access_key_id='{credentials['aws_access_key_id']}'")
            self.conn.execute(f"SET s3_secret_access_key='{credentials['aws_secret_access_key']}'")
            self.conn.execute(f"SET s3_session_token='{credentials['aws_session_token']}'")
            logger.debug("AWS credentials updated in DuckDB")
        
    def execute_query(self, query: str, params: Optional[Dict] = None) -> Tuple[pd.DataFrame, Dict]:
        """Execute a query with DuckDB. Thread-safe."""
        with self._conn_lock:
            self._update_credentials()
            
            start_time = time.time()
            
            try:
                logger.debug(f"Executing query: {query}")
                if params:
                    result = self.conn.execute(query, params).df()
                else:
                    result = self.conn.execute(query).df()
                
                query_time = time.time() - start_time
                metrics = {"query_time_ms": round(query_time * 1000, 2)}
                
                logger.debug(f"Query completed in {metrics['query_time_ms']}ms")
                return result, metrics
                
            except Exception as e:
                logger.error(f"Query execution failed: {str(e)}")
                raise


# Create the DB manager once at application startup
db_manager = DuckDBManager(
    role_arn=config['role_arn'],
    region=config['region']
)     

# Set the threadpool with proper management
THREAD_POOL_SIZE = 4
thread_pool = ThreadPoolExecutor(max_workers=THREAD_POOL_SIZE)

def execute_query(query: str, params: Optional[Dict] = None) -> pd.DataFrame:
    """Execute a query and return the result DataFrame."""
    try:
        logger.debug(f"Executing query: {query[:100]}{'...' if len(query) > 100 else ''}")
        result, metrics = db_manager.execute_query(query, params)
        
        logger.info(f"Query metrics: {metrics}")
      
        if not result.empty:
            return result
        else:
            logger.warning("Query returned empty result")
            return pd.DataFrame()

    except Exception as e:
        logger.error(f"Error executing query: {str(e)}")
        raise             
            
            
def generate_lookup_condition(df: pd.DataFrame, columns: List[str]) -> str:
    """Generate SQL WHERE condition from DataFrame columns."""
    if df.empty or not columns:
        logger.error("Cannot generate lookup condition: empty DataFrame or no columns specified")
        raise ValueError("Empty DataFrame or no columns specified")
        
    try:
        conditions = []
        for column in columns:
            if column not in df.columns:
                logger.error(f"Column '{column}' not found in DataFrame")
                raise ValueError(f"Column '{column}' not in DataFrame")
                
            value = df[column].iloc[0]
            # Handle different data types appropriately
            if isinstance(value, str):
                conditions.append(f"{column}='{value}'")
            else:
                conditions.append(f"{column}={value}")
                
        condition_str = " AND ".join(conditions)
        logger.debug(f"Generated lookup condition: {condition_str}")
        return condition_str
    except Exception as e:
        logger.error(f"Error generating lookup condition: {str(e)}")
        raise
    
def fetch_data_from_db(lookup_condition: str, s3folder: str, projectkey: str, bucket: str) -> pd.DataFrame:
    """Fetch data from S3 via DuckDB using the provided lookup condition."""
    start_time = time.time()
    
    logger.info(f"Fetching data with lookup condition: {lookup_condition}")
    logger.debug(f"S3 folder: {s3folder}, Project key: {projectkey}, Bucket: {bucket}")
    
    query = f"""
        SELECT * 
        FROM read_parquet('s3://{bucket}/dataiku/{projectkey}/{s3folder}/*.parquet') 
        WHERE {lookup_condition} 
        LIMIT 1
    """
    
    try:
        logger.debug(f"Executing query: {query}")
        result = execute_query(query)
        
        if result.empty:
            logger.warning(f"No data found for condition: {lookup_condition}")
        else:
            logger.info(f"Successfully fetched data: {len(result)} rows")
            
        return result
    except Exception as e:
        logger.error(f"Error fetching precomputed feature: {str(e)}")
        raise RuntimeError(f"Error while fetching precomputed feature: {str(e)}")
    finally:    
        query_time = time.time() - start_time
        logger.info(f"Total time to get precomputed feature: {round(query_time * 1000, 2)}ms")

def execute_in_thread(fetch_functions: List[Tuple], request_id: str) -> List[Optional[pd.DataFrame]]:
    """Execute multiple functions in parallel threads with proper error handling."""
    logger.info(f"[{request_id}] Starting parallel execution of {len(fetch_functions)} functions")
    
    results = [None] * len(fetch_functions)
    futures = []
    
    try:
        # Submit all tasks to the thread pool
        for index, (func, lookup_condition, s3folder, projectkey, bucket) in enumerate(fetch_functions):
            logger.debug(f"[{request_id}] Submitting task {index} to thread pool")
            future = thread_pool.submit(func, lookup_condition, s3folder, projectkey, bucket)
            futures.append((future, index))
        
        # Wait for completion with timeout
        for future, index in futures:
            try:
                results[index] = future.result(timeout=10)  # Increased timeout from 5 to 10 seconds
                logger.info(f"[{request_id}] Task {index} completed successfully")
            except TimeoutError:
                logger.error(f"[{request_id}] Task {index} timed out")
                results[index] = None
            except Exception as e:
                logger.error(f"[{request_id}] Task {index} failed with error: {str(e)}")
                results[index] = None
                
        return results
    except Exception as e:
        logger.error(f"[{request_id}] Fatal error in thread execution: {str(e)}")
        return results

def trestleiq_call() -> pd.DataFrame:
    """Call external REST API and return results as DataFrame."""
    url = "https://api.test.com/getme"
    payload = {}
    headers = {
        'x-api-key': 'testkey',
        'Accept': 'application/json'
    }
    
    logger.info(f"Making API call to {url}")
    start_time = time.time()
    
    try:
        response = requests.request("GET", url, headers=headers, data=payload, timeout=10)
        response.raise_for_status()  # Raise exception for non-200 status codes
        
        data = response.json()
        df = pd.DataFrame([data])
        
        api_time = time.time() - start_time
        logger.info(f"API call successful, took {round(api_time * 1000, 2)}ms")
        logger.debug(f"API response data: {df.head()}")
        
        return df
    except requests.exceptions.RequestException as e:
        logger.error(f"API call failed: {str(e)}")
        return pd.DataFrame()
    except ValueError as e:
        logger.error(f"Failed to parse API response as JSON: {str(e)}")
        return pd.DataFrame()
    finally:
        logger.debug(f"API call completed in {round((time.time() - start_time) * 1000, 2)}ms")

def model_api_endpoint(features: Dict[str, Any]) -> Dict[str, Any]:
    """Main API endpoint to process features and return model predictions."""
    request_id = f"req-{int(time.time())}"
    logger.info(f"[{request_id}] Processing request with {len(features) if features else 0} features")
    
    if not features:
        logger.warning(f"[{request_id}] No features provided in input data")
        return {"error": "No features provided in input data"}
    
    try:
        input_df = pd.DataFrame([features])
        logger.debug(f"[{request_id}] Input features: {input_df.columns.tolist()}")
        
        bucket = config['bucket']
        projectkey = config['projectkey']
        s3folder = config['s3folder']
        
        # Prepare functions to execute in parallel
        fetch_functions = [
            (fetch_data_from_db, generate_lookup_condition(input_df, ["key1"]), s3folder, projectkey, bucket),
            # (fetch_data_from_db, generate_lookup_condition(input_df, ["key2"]), s3folder, projectkey, bucket),
            # (fetch_data_from_db, generate_lookup_condition(input_df, ["key3"]), s3folder, projectkey, bucket),
            (fetch_data_from_db, generate_lookup_condition(input_df, ["key4"]), s3folder, projectkey, bucket)
        ]

        # Execute functions in parallel
        results = execute_in_thread(fetch_functions, request_id)
        
        # Check for errors in results
        if any(result is None for result in results):
            logger.error(f"[{request_id}] One or more threads failed or didn't return any data")
            return {"status": "error", "message": "Failed to compute required features"}
        
        # Process results
        if any(df.empty for df in results):
            logger.warning(f"[{request_id}] One or more queries returned empty results")
            return {"status": "error", "message": "One or more features returned no data"}
      
        # Merge all features
        try:
            merged_df = pd.concat(results, axis=1)
            final_df = pd.concat([merged_df, input_df], axis=1)
            
            # Remove duplicate columns if any
            final_df = final_df.loc[:, ~final_df.columns.duplicated()]
            
            logger.debug(f"[{request_id}] Final feature set shape: {final_df.shape}")
            
            # Make prediction
            class_predictions = model.predict(final_df)
            prediction_value = int(class_predictions[0])
            
            logger.info(f"[{request_id}] Prediction successful: {prediction_value}")
            
            return {
                "status": "success",
                "predictions": prediction_value,
                "request_id": request_id
            }
        except Exception as e:
            logger.error(f"[{request_id}] Error during prediction: {str(e)}")
            return {
                "status": "error", 
                "message": str(e),
                "request_id": request_id
            }
            
    except Exception as e:
        logger.error(f"[{request_id}] Unexpected error in API endpoint: {str(e)}")
        return {
            "status": "error",
            "message": f"Unexpected error: {str(e)}",
            "request_id": request_id
        }

# Application cleanup function
def cleanup():
    """Clean up resources when application is shutting down."""
    logger.info("Shutting down application and cleaning up resources")
    thread_pool.shutdown(wait=True)
    logger.info("Thread pool shut down successfully")

# Register cleanup function to be called on application exit
import atexit
atexit.register(cleanup)
