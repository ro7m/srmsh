import dataiku
from dataikuapi.dss.project import DSSProject
import dataikuapi
import pandas as pd
import numpy as np
from typing import Dict, List, Any
import sys
import dataikuscoring 
import requests
import os
import time
import duckdb
from typing import Optional, Dict
from datetime import datetime,timezone
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

# Insert here initialization code

model_zip_folder = folders[0]

sys.path.append(model_zip_folder)

model = dataikuscoring.load_model(f"{model_zip_folder}/weights.zip")

config = {
     'role_arn': 'arn:aws:iam::975050017269:role/SL-ROL-DKU-UAT-NV-DSS',
     'region': 'us-east-1',
     'bucket': 'mybuk',
     'projectkey':'myMODEL',
     's3folder': 'V2_VW_prepared',
}

class STSManager:
    def __init__(self, role_arn: str, region: str = 'us-east-1', session_name: str = 'DuckDBSession'):
        self.role_arn = role_arn
        self.region = region
        self.session_name = session_name
        self.credentials = None
        self.expiration = None
    
    def get_credentials(self):
        if self._should_refresh(): 
            print("calling remote for creds...")
            client = dataikuapi.DSSClient("http://x.y.x","mytempo")
            project_key = config['projectkey']
            runnable_type="pyrunnable_testrole_rolefetcher"
            project=DSSProject(client,project_key)
            macro=project.get_macro(runnable_type)
            run_id= macro.run(wait=True)
            self.credentials = macro.get_result(run_id,as_type='json')
            self.expiration = self.credentials['expiration']
            print("got creds")

        return {
            'aws_access_key_id': self.credentials['aws_access_key_id'],
            'aws_secret_access_key': self.credentials['aws_secret_access_key'],
            'aws_session_token': self.credentials['aws_session_token'],
            'expiration': self.credentials['expiration']
        }
    
    def _should_refresh(self):
        if not self.credentials :
            print("from if",self.credentials)
            return True
        now = datetime.now(timezone.utc).isoformat()

        time_remaining = datetime.fromisoformat(self.expiration) - datetime.fromisoformat(now)
        print("time_remaining for token expiry : ",time_remaining.total_seconds())
        return time_remaining.total_seconds() < 60  # Refresh if less than 1 minutes remaining

class DuckDBManager:
    def __init__(self, role_arn: str, region: str = 'us-east-1', cache_size_mb: int = 2000):
        self.conn = duckdb.connect(database=':memory:')
        self.sts_manager = STSManager(role_arn, region)
        self._setup_connection(cache_size_mb)
        
    def _setup_connection(self, cache_size_mb: int):
        self.conn.install_extension(f"{model_zip_folder}/httpfs.duckdb_extension")
        self.conn.execute("LOAD httpfs")
        self.conn.execute(f"SET memory_limit='{cache_size_mb}MB'")
        self.conn.execute("SET enable_object_cache=true")
        self._update_credentials()
    
    def _update_credentials(self):
        credentials = self.sts_manager.get_credentials()        
        self.conn.execute(f"SET s3_region='{self.sts_manager.region}'")
        self.conn.execute(f"SET s3_access_key_id='{credentials['aws_access_key_id']}'")
        self.conn.execute(f"SET s3_secret_access_key='{credentials['aws_secret_access_key']}'")
        self.conn.execute(f"SET s3_session_token='{credentials['aws_session_token']}'")
        
    def execute_query(self, query: str, params: Optional[Dict] = None) -> tuple[pd.DataFrame, Dict]:
        
        self._update_credentials()
        
        start_time = time.time()
        
        try:
            if params:
                result = self.conn.execute(query, params).df()
            else:
                result = self.conn.execute(query).df()
            
            query_time = time.time() - start_time
            
            metrics = {
                "query_time_ms": round(query_time * 1000, 2)
            }
            
            return result, metrics
            
        except Exception as e:
            print(f"Query execution failed: {str(e)}")
            raise


# Create the DB manager once at application startup
db_manager = DuckDBManager(
    role_arn=config['role_arn'],
    region=config['region']
)     

#set the threadpool to 4 workers
executor = ThreadPoolExecutor(max_workers=4)

def execute_query(query: str, params: Optional[Dict] = None):
    try:
        result, metrics = db_manager.execute_query(query, params)
        
        print("metrics -->",metrics)
      
        if (not result.empty):
            return result
        else:
            return pd.DataFrame()

    except Exception as e:
        print(str(e))
        raise             
            
            
def generate_lookup_condition(df, columns):
    conditions = []
    for column in columns:
        value = df[column].iloc[0]
        conditions.append(f"{column}={value}")
        return " and ".join(conditions)
    
def execute_in_thread(fetch_functions, results):
    
    try:
        future_to_index = {executor.submit(func, lookup_condition,s3folder,projectkey,bucket): index for index, (func, lookup_condition,s3folder,projectkey,bucket) in enumerate(fetch_functions)}
        try:
            for future in as_completed(future_to_index, timeout=5):
                index = future_to_index[future]
                print(f"Thread {threading.current_thread().name} is executing task = {index}")
                results[index] = future.result()
        except TimeoutError:
            print(f"Thread {threading.current_thread().name} timedout task = {index}")
            results[index] = None
        except Exception as e:
            print(f"Thread {threading.current_thread().name} errored out with: {str(e)} task = {index}")
            results[index] = None
    except Exception as e:
        return {"error": "features couldnt be computed"}
    
def model_api_endpoint(features : Dict[str, Any]) -> Dict[str, Any]:
      
    if not features:
        return {"error": "No features provided in input data"}
    
    input_df = pd.DataFrame([features])
    
    results = [None,None]
    
    bucket = config['bucket']
    projectkey = config['projectkey']
    rcvr_s3folder = config['s3folder']
    
    fetch_functions = [
                        (fetch_data_from_db, generate_lookup_condition(input_df,["key1"]),s3folder,projectkey,bucket),
                        #(fetch_data_from_db, generate_lookup_condition(input_df,["key2"]),s3folder,projectkey,bucket),
                        #(fetch_data_from_db, generate_lookup_condition(input_df,["key3"]),s3folder,projectkey,bucket),
                        (fetch_data_from_db, generate_lookup_condition(input_df,["key4"]),s3folder,projectkey,bucket)
                       ]

    execute_in_thread(fetch_functions,results)               
    
    if any(result is None for result in results):
        print("One or more threads failed or didn't return any data")
        return {"status":"error","message": "Failed to compute required features"}       
      
    try:
        merged_df = pd.concat(results, axis=1)
    
        final_df = pd.concat([merged_df, input_df], axis=1)
        
        print(final_df.to_string())
        
        class_predictions = model.predict(input_df)[0] 
        
        return {
            "status": "success",
            "predictions": int(class_predictions),
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e)
        }

def trestleiq_call():
    url = "https://api.test.com/getme"
    payload = {}
    headers = {
        'x-api-key': 'testkey',
        'Accept': 'application/json'
    }
    try:
        response = requests.request("GET", url, headers=headers, data=payload, timeout=10)
        data = response.json()
        df = pd.DataFrame([data])
        print("API response -->",df)
        return df
    except requests.exceptions.RequestException as e:
        print(f"API call failed: {e}")
        return pd.DataFrame()
 

def fetch_data_from_db(lookup_condition,s3folder,projectkey,bucket):
    
    start_time = time.time()
    
    print("recieved lookup_condition --> ", lookup_condition)
    print("s3folder ->",s3folder)
    print("projectkey ->",projectkey)
    print("bucket ->",bucket)
    
    
    query = f""" SELECT * FROM read_parquet('s3://{bucket}/dataiku/{projectkey}/{s3folder}/*.parquet') WHERE {lookup_condition} LIMIT 1"""
    
    try:
        print("executing query: ", query)
        out = execute_query(query) 
        print("out -->",out)
    except Exception as e:
        print(str(e))
        raise RuntimeError("error while fetching precomputed feature")
    finally:    
        query_time = time.time() - start_time
        print("total_time_ms to get precomputed feature",round(query_time * 1000, 2))
        
    return out 
